
# ğŸ¦™ Llama Chat - Interface Web pour Ollama

Une interface de chat moderne et Ã©lÃ©gante pour interagir avec les modÃ¨les Llama via Ollama. CrÃ©ez plusieurs conversations, gÃ©rez vos topics et profitez d'une expÃ©rience utilisateur fluide avec animations et notifications.

![Llama Chat Preview](https://img.shields.io/badge/Next.js-13+-black.svg)
![Ollama](https://img.shields.io/badge/Ollama-Compatible-green.svg)
![Tailwind CSS](https://img.shields.io/badge/Tailwind-CSS-blue.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)
![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)

---

## ğŸš€ DÃ©marrage rapide avec Docker

### 1. Cloner le dÃ©pÃ´t

```bash
git clone <votre-repo>
cd chat-visualizer
```

### 2. DÃ©marrer les services Docker

> ğŸ“¦ **PrÃ©requis** : Assurez-vous d'avoir Docker et Docker Compose installÃ©s sur votre machine

```bash
docker-compose up --build
```

Cela va construire et dÃ©marrer les deux conteneurs nÃ©cessaires :
- **Ollama** : Serveur local pour les modÃ¨les LLM (accessible sur [http://localhost:11434](http://localhost:11434))
- **PostgreSQL** : Base de donnÃ©es pour stocker les conversations et topics (accessible sur [http://localhost:5432](http://localhost:5432))

> ğŸ”„ **Note importante** : Lors du premier dÃ©marrage, **le modÃ¨le `llama3` est tÃ©lÃ©chargÃ© automatiquement** via `ollama pull llama3`.  
> Ce tÃ©lÃ©chargement peut prendre plusieurs minutes (~4 Ã  8 Go selon la version du modÃ¨le).  
>  
> ğŸ“º Pour suivre l'avancÃ©e, ouvrez un autre terminal et utilisez la commande suivante :

```bash
docker-compose logs -f ollama
```

Cela vous montrera la progression du tÃ©lÃ©chargement ligne par ligne.

### 3. Lancer l'application Next.js

Dans un nouveau terminal :

```bash
cd webapp
npm install
npx prisma generate     # GÃ©nÃ¨re les types Prisma
npx prisma db push      # Pousse le schÃ©ma Ã  la base PostgreSQL
npm run dev             # DÃ©marrage en mode dÃ©veloppement
```

> ğŸ’¡ L'application sera disponible sur [http://localhost:3000](http://localhost:3000)

---

## ğŸ› ï¸ Architecture du projet

```
chat-visualizer/
â”œâ”€â”€ docker-compose.yml        # Configuration des conteneurs
â”œâ”€â”€ README.md                 # Documentation principale
â””â”€â”€ webapp/                   # Application Next.js
    â”œâ”€â”€ prisma/               # SchÃ©ma Prisma + client DB
    â”œâ”€â”€ public/               # Fichiers statiques
    â”œâ”€â”€ src/                  # Source de l'app Next.js
    â”œâ”€â”€ package.json
    â”œâ”€â”€ next.config.ts
    â””â”€â”€ ...
```

---

## ğŸ”§ Scripts disponibles (`webapp/package.json`)

```json
"scripts": {
  "dev": "next dev",                    // DÃ©marrage en mode dÃ©veloppement
  "build": "next build",                // Build pour la production
  "start": "next start",                // DÃ©marrage en production
  "lint": "next lint",                  // Analyse de code
  "resetdb": "prisma migrate reset --force",    // RÃ©initialisation de la base (âš  destructif)
  "updatedb": "npx prisma migrate dev --name",  // GÃ©nÃ©rer une migration Prisma
  "showdb": "npx prisma studio"        // Ouvre Prisma Studio dans le navigateur
}
```

Les scripts sont Ã  utiliser dans le rÃ©pertoire `webapp` :

```bash
cd webapp
npm run dev          # DÃ©marrer l'application
npm run build        # Construire pour la production
npm run start        # DÃ©marrer en mode production
npm run lint         # Linter le code
npm run resetdb      # RÃ©initialiser la base de donnÃ©es (attention, cela supprime toutes les donnÃ©es !)
npm run updatedb     # Mettre Ã  jour la base de donnÃ©es avec les derniÃ¨res migrations
npm run showdb       # Ouvrir Prisma Studio pour visualiser la base de donnÃ©es
```

Le script 'updatedb' est Ã  utiliser pour gÃ©nÃ©rer une migration aprÃ¨s avoir modifiÃ© le schÃ©ma Prisma. Il prend un argument pour nommer la migration, par exemple :

```bash
npm run updatedb -- --name "ajout_colonne_exemple"
```

Le script 'resetdb' est destructif et supprime toutes les donnÃ©es de la base. Utilisez-le avec prÃ©caution, surtout en production.

Le script 'showdb' ouvre Prisma Studio, une interface graphique pour interagir avec la base de donnÃ©es. C'est trÃ¨s utile pour visualiser et modifier les donnÃ©es directement depuis le navigateur.

---

## ğŸ§ª Technologies utilisÃ©es

- **Next.js 13+** â€“ Framework fullstack React
- **React 18+**
- **Tailwind CSS** â€“ Design responsive et moderne
- **Prisma ORM** â€“ Mapping PostgreSQL
- **Ollama API** â€“ Serveur local de LLMs
- **Docker Compose** â€“ Orchestration locale des services

---

## ğŸ› DÃ©pannage

### ğŸ“¦ Le modÃ¨le `llama3` n'est pas encore prÃªt ?

- Soyez patient lors du premier lancement.
- Suivez les logs de tÃ©lÃ©chargement via :

```bash
docker-compose logs -f ollama
```

- Le modÃ¨le sera prÃªt lorsque vous verrez quelque chose comme :

```
âœ“ llama3 pulled successfully
```

Si jamais le tÃ©lÃ©chargement Ã©choue, essayez de relancer la commande :

```bash
docker-compose up --build
```

Ou bien essayez de tÃ©lÃ©charger manuellement le modÃ¨le avec :

```bash
docker exec -it ollama ollama pull llama3
```

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Pousser vers la branche
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ”— Liens utiles

- [Documentation Ollama](https://ollama.ai/docs)
- [ModÃ¨les Llama disponibles](https://ollama.ai/library)
- [API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Next.js Documentation](https://nextjs.org/docs)
- [Tailwind CSS](https://tailwindcss.com)

---

**DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© IA**